"""
Test that functions refactored for numba yield consistent results.

Reference: https://numba.readthedocs.io/en/stable/user/jit.html
"""
from itertools import zip_longest

import numpy as np

from fos.field import load_fields
from fos.vectors import vectorize, embed_tfidf


def test_load_vocab(en_vocab, en_dict):
    assert len(en_vocab.token_to_id) == len(en_dict.token2id)
    for k, v in en_vocab.token_to_id.items():
        assert isinstance(k, str)
        assert isinstance(v, int)
        assert en_dict.token2id[k] == v

    assert len(en_vocab.id_to_token) > 0
    for k, v in en_vocab.id_to_token.items():
        assert isinstance(k, int)
        assert isinstance(v, str)

    assert len(en_vocab.id_to_frequency) > 0
    for k, v in en_vocab.id_to_frequency.items():
        assert isinstance(k, int)
        assert isinstance(v, int)

    assert len(en_vocab.id_to_idf) > 0
    for k, v in en_vocab.id_to_idf.items():
        assert isinstance(k, int)
        assert isinstance(v, float)


def test_to_bow(en_vocab, en_dict, texts):
    for text in texts:
        bow = en_vocab.to_bow(text)
        gensim_bow = en_dict.doc2bow(text.split())
        gensim_bow = dict(gensim_bow)
        assert len(bow) == len(gensim_bow)
        for k, v in bow.items():
            assert gensim_bow[k] == v


def test_embed_tfidf(en_vocab, en_tfidf, texts):
    tfidf, dictionary = en_tfidf
    for text in texts.values():
        vector = vectorize(text, en_vocab, eps=1e-12)
        vector = sorted(vector, key=lambda x: x[0])
        gensim_vector = embed_tfidf(text.split(), tfidf, dictionary)
        assert len(vector) == len(gensim_vector)
        for (k, v), (i, x) in zip_longest(vector, gensim_vector):
            assert k == i
            if v - x > 1e-6:
                print(k, v, x)


def test_load_fields():
    fields = load_fields('en')
    assert isinstance(fields.entity, np.ndarray)
    assert isinstance(fields.fasttext, np.ndarray)
    assert isinstance(fields.tfidf, np.ndarray)
    assert fields.entity.shape == fields.fasttext.shape
    assert fields.entity.shape[0] == fields.tfidf.shape[0]


def test_embed(en_tfidf, texts):
    tfidf, dictionary = en_tfidf
    for text in texts.values():
        bow = dictionary.doc2bow(text.split())
        vector = tfidf.gensim_model[bow]

    # vector = [(1, 31.91950317744806), (2, 205.85204486043955), (3, 75.58173023138923), (4, 147.18331835751607), (5, 79.50638659194918), (8, 12.601030031931934), (9, 14.547149697141332), (10, 34.58061916794117), (11, 13.461030913489004), (16, 7.366526954872669), (17, 137.11482711186812), (19, 37.158376559229765), (20, 84.38437339678417), (22, 30.69965788641648), (23, 19.672045373554344), (24, 241.4606898715344), (27, 4.951010247329379), (28, 56.95678744843285), (29, 6.16844635082109), (31, 217.84057568878129), (35, 60.96336850999908), (36, 3.5741474765210146), (37, 59.75922443327539), (38, 77.47257344454698), (39, 23.186719374547394), (40, 9.821616298507404), (43, 11.660414377169378), (44, 34.398584911906056), (45, 23.800001701943966), (47, 11.11501120634517), (48, 19.160228691350195), (49, 18.43134684615002), (51, 5.426703766909131), (54, 373.5872837732965), (55, 198.60169851710418), (56, 13.70249764489032), (57, 23.411103387558992), (58, 12.681428970169007), (59, 12.539132299672094), (60, 113.39536656371413), (61, 42.12140680506403), (67, 9.763485991443885), (68, 8.45186054040315), (72, 19.18605192528878), (74, 5.410421840603306), (75, 33.16659491209533), (77, 60.291235787754594), (78, 48.16233171093132), (79, 8.249337690537377), (80, 5.751599090168946), (82, 7.606227688829871), (83, 30.13643742071402), (84, 28.96968048519177), (86, 15.905876867239447), (92, 4.625310721563559), (93, 62.45722904346026), (94, 6.801807209545228), (95, 28.7173386278927), (96, 11.88490284000815), (98, 10.745962676433841), (99, 75.76568501722382), (100, 27.102463520773114), (101, 10.49434450631866), (103, 58.1441954778449), (105, 208.42967713391764), (107, 12.290352315157994), (108, 158.11218241116674), (109, 23.07350791485191), (110, 34.74520695488493), (111, 38.921709430920224), (112, 7.74328275281838), (117, 24.680284167660357), (118, 5.449884598949288), (121, 21.321287109432504), (122, 68.1319107001371), (125, 45.914467654882024), (127, 81.25333214866873), (129, 16.877044245488133), (131, 29.55491730649784), (137, 18.74164163755783), (138, 13.82960802694223), (139, 49.7524352700555), (140, 33.385612731710424), (141, 7.288454468696928), (143, 21.06904460571311), (144, 71.23636577993365), (146, 97.28050503733047), (147, 12.539169268097398), (148, 37.44198550928517), (149, 80.52893605731174), (150, 14.872007397449154), (156, 53.190731332630754), (157, 152.6646384193122), (159, 38.72367997979468), (160, 16.784485000579284), (161, 14.284021060076748), (166, 57.34851368439024), (167, 72.26030907638372), (168, 23.053542282902416), (171, 20.23688248585619...
    # n_n = 'c'
    # pivot = None, so we normalize like 
    # length = 1.0 * math.sqrt(sum(val ** 2 for _, val in vec))
    # = 2254.7968658617724
    # then norm_vector is 
    # [(1, 0.014156265542460997), (2, 0.09129516187337962), (3, 0.03352041657309217), (4, 0.06527564437662252), (5, 0.03526099747418365), (8, 0.00558854335071815), (9, 0.006451645342154351), (10, 0.015336467639946194), (11, 0.005969952822488187), (16, 0.003267046830871489), (17, 0.06081027927075085), (19, 0.01647970028778092), (20, 0.03742437940835654), (22, 0.013615265459703938), (23, 0.008724531096966814), (24, 0.1070875578759727), (27, 0.002195767752869005), (28, 0.025260274355873847), (29, 0.002735699363527162), (31, 0.09661206248196716), (35, 0.02703718877429749), (36, 0.001585130585656989), (37, 0.026503152163303945), (38, 0.03435900351712496), (39, 0.010283285259794587), (40, 0.004355876330683842), (43, 0.005171381313195512), (44, 0.015255735641959518), (45, 0.010555275316496292), (47, 0.004929495589881914), (48, 0.00849754094545774), (49, 0.008174282626167147), (51, 0.0024067373203639215), (54, 0.16568556104964838), (55, 0.08807964101954682), (56, 0.00607704305977616), (57, 0.010382799329735355), (58, 0.005624200193892955), (59, 0.005561091772619481), (60, 0.05029072386987507), (61, 0.018680798897139424), (67, 0.004330095601632978), (68, 0.0037483911160098625), (72, 0.008508993522108682), (74, 0.0023995163034500095), (75, 0.014709349393839615), (77, 0.0267390986303822), (78, 0.02135994263612918), (79, 0.003658572448558252), (80, 0.0025508280489695964), (82, 0.0033733538501805604), (83, 0.013365477785155613), (84, 0.012848022331324156), (86, 0.0070542393898353675), (92, 0.002051320361311478), (93, 0.027699714324194526), (94, 0.0030165942274119717), (95, 0.012736108987324263), (96, 0.005270941706522995), (98, 0.004765822961318862), (99, 0.03360200032399217), (100, 0.012019913603354545), (101, 0.00465423057181152), (103, 0.025786888547772783), (105, 0.09243833903159912), (107, 0.0054507581153926616), (108, 0.07012258390324533), (109, 0.010233076098424203), (110, 0.015409462147538279), (111, 0.017261736531660708), (112, 0.0034341376245699785), (117, 0.010945679649163284), (118, 0.0024170179945972067), (121, 0.009455968044058644), (122, 0.030216429573622552), (125, 0.02036301732987097), (127, 0.03603576596138921), (129, 0.007484951084069299), (131, 0.01310757423605079), (137, 0.008311898034502042), (138, 0.006133416378355937), (139, 0.022065151865039673), (140, 0.014806483562744622), (141, 0.0032324217666992883), (143, 0.009344098763265144), (144, 0.03159325208335672), (146, 0.04314380000707972), (147, 0.00556110816807659), (148, 0.01660548055399883), (149, 0.03571449706913353), (150, 0.00659571938502103), (156, 0.023590032493814698), (157, 0.06770660396539292), (159, 0.01717391068174768), (160, 0.007443901157882946), (161, 0.006334948072857758), (166, 0.02543400452282956), (167, 0.03204736984090413), (168, 0.010224221361994605), (171, 0.008975035752554034...
    # then pivot is None so norm_vector is 
    # [(1, 0.014156265542460997), (2, 0.09129516187337962), (3, 0.03352041657309217), (4, 0.06527564437662252), (5, 0.03526099747418365), (8, 0.00558854335071815), (9, 0.006451645342154351), (10, 0.015336467639946194), (11, 0.005969952822488187), (16, 0.003267046830871489), (17, 0.06081027927075085), (19, 0.01647970028778092), (20, 0.03742437940835654), (22, 0.013615265459703938), (23, 0.008724531096966814), (24, 0.1070875578759727), (27, 0.002195767752869005), (28, 0.025260274355873847), (29, 0.002735699363527162), (31, 0.09661206248196716), (35, 0.02703718877429749), (36, 0.001585130585656989), (37, 0.026503152163303945), (38, 0.03435900351712496), (39, 0.010283285259794587), (40, 0.004355876330683842), (43, 0.005171381313195512), (44, 0.015255735641959518), (45, 0.010555275316496292), (47, 0.004929495589881914), (48, 0.00849754094545774), (49, 0.008174282626167147), (51, 0.0024067373203639215), (54, 0.16568556104964838), (55, 0.08807964101954682), (56, 0.00607704305977616), (57, 0.010382799329735355), (58, 0.005624200193892955), (59, 0.005561091772619481), (60, 0.05029072386987507), (61, 0.018680798897139424), (67, 0.004330095601632978), (68, 0.0037483911160098625), (72, 0.008508993522108682), (74, 0.0023995163034500095), (75, 0.014709349393839615), (77, 0.0267390986303822), (78, 0.02135994263612918), (79, 0.003658572448558252), (80, 0.0025508280489695964), (82, 0.0033733538501805604), (83, 0.013365477785155613), (84, 0.012848022331324156), (86, 0.0070542393898353675), (92, 0.002051320361311478), (93, 0.027699714324194526), (94, 0.0030165942274119717), (95, 0.012736108987324263), (96, 0.005270941706522995), (98, 0.004765822961318862), (99, 0.03360200032399217), (100, 0.012019913603354545), (101, 0.00465423057181152), (103, 0.025786888547772783), (105, 0.09243833903159912), (107, 0.0054507581153926616), (108, 0.07012258390324533), (109, 0.010233076098424203), (110, 0.015409462147538279), (111, 0.017261736531660708), (112, 0.0034341376245699785), (117, 0.010945679649163284), (118, 0.0024170179945972067), (121, 0.009455968044058644), (122, 0.030216429573622552), (125, 0.02036301732987097), (127, 0.03603576596138921), (129, 0.007484951084069299), (131, 0.01310757423605079), (137, 0.008311898034502042), (138, 0.006133416378355937), (139, 0.022065151865039673), (140, 0.014806483562744622), (141, 0.0032324217666992883), (143, 0.009344098763265144), (144, 0.03159325208335672), (146, 0.04314380000707972), (147, 0.00556110816807659), (148, 0.01660548055399883), (149, 0.03571449706913353), (150, 0.00659571938502103), (156, 0.023590032493814698), (157, 0.06770660396539292), (159, 0.01717391068174768), (160, 0.007443901157882946), (161, 0.006334948072857758), (166, 0.02543400452282956), (167, 0.03204736984090413), (168, 0.010224221361994605), (171, 0.008975035752554034...
